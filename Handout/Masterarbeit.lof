\select@language {ngerman}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Vogelperspektive auf eine LiDAR-Punktwolke, wobei die roten Punkte alle Boden- und die blauen alle Nicht-Bodenpunkte darstellen\relax }}{3}{figure.caption.7}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Labeling Methoden in C.LABEL\relax }}{6}{figure.caption.8}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Ablauf einer LiDAR-Messung von einem Sensor, der die Messimpulse im Uhrzeigersinn abgibt. Die Bilder stammen von \cite {bib:LidarPictures} und \ref {fig:Lidar2} wurde um den Winkel $\varphi $ erweitert.\relax }}{9}{figure.caption.9}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{9}{subfigure.1.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{9}{subfigure.1.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{9}{subfigure.1.3}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Seitliche Sicht auf eine LiDAR-Messung mit mehreren, verital angeordneten Lasern. Die Abbildung stammt aus \cite {bib:FastGroundSeg} und wurde durch die Einzeichnung des Winkels $\theta $ erweitert.\relax }}{9}{figure.caption.10}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Beispiele des Label-Prozess in Sensordaten\relax }}{11}{figure.caption.11}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Klassifizierung von Autos auf einem Kamerabild}}}{11}{subfigure.3.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Klassifizierung von Objekten in einer Punktwolke \cite {bib:LidarDetection}}}}{11}{subfigure.3.2}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Diagramm um den besten Machine Learning Algorithmus für eine \gls {glos:PredAna}-Methoden zu finden \cite {bib:AzureCs}\relax }}{12}{figure.caption.12}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Abbildung des Übertragungsprinzips von Reizen zwischen Neuronen \cite {bib:Neurons}\relax }}{14}{figure.caption.13}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Einfaches Perzeptron nach dem Prinzip von Frank Rosenblatt\relax }}{15}{figure.caption.14}
\contentsline {figure}{\numberline {2.7}{\ignorespaces Stark vereinfachte Darstellung eines \acrshort {acr:knn}s zur Erkennung von Autos und Personen\relax }}{16}{figure.caption.15}
\contentsline {figure}{\numberline {2.8}{\ignorespaces Die Abbildungen zeigen zwei Punktwolken, wie sie in C.LABEL visualisiert werden.\relax }}{19}{figure.caption.16}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Ungelabelte Punktwolke}}}{19}{subfigure.8.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Gelabelte Punktwolke}}}{19}{subfigure.8.2}
\contentsline {figure}{\numberline {2.9}{\ignorespaces Vergleich zwischen den Anfängen der VR-Technologie und dem heutigen Stand. Die oberen drei Bilder zeigen das VIVED-System, das von S.S. Fisher vorgestellt wurde \cite {bib:NasaVr}. Die Unteren zeigen Bilder der Oculus Rift.\relax }}{21}{figure.caption.17}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {VIVED Displayaufbau}}}{21}{subfigure.9.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Sicht auf einen VIVED Benutzer}}}{21}{subfigure.9.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Sicht eines VIVED Benutzers}}}{21}{subfigure.9.3}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {Rift Displayaufbau \cite {bib:RiftReview}}}}{21}{subfigure.9.4}
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {Sicht auf einen Rift Benutzer \cite {bib:RiftUser}}}}{21}{subfigure.9.5}
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {Sicht eines Rift Benutzers \cite {bib:AvatarHands}}}}{21}{subfigure.9.6}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Die zwei potentiellen \acrshort {acr:AR}-Brillen\relax }}{24}{figure.caption.18}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Microsoft Hololens \acrshort {acr:AR}-Brille}}}{24}{subfigure.1.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Meta 2 \acrshort {acr:AR}-Brille}}}{24}{subfigure.1.2}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Die zwei potentiellen \acrshort {acr:VR}-Brillen\relax }}{28}{figure.caption.19}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {HTC Vive \acrshort {acr:VR}-Brille mit Controllern \cite {bib:VivePic}}}}{28}{subfigure.2.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Oculus Rift \acrshort {acr:VR}-Brille mit Controllern \cite {bib:OculusPic}}}}{28}{subfigure.2.2}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Unity Editor, der C.LABEL-VR im Play Mode ausführt.\relax }}{33}{figure.caption.21}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces \glslink {glos:Workflow} in C.LABEL-VR vom Import bis zum Export\relax }}{38}{figure.caption.22}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Das Import-Export-Prinzip in C.LABEL-VR\relax }}{42}{figure.caption.23}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Die Abbildungen zeigen Beispiele für die Inhalte einer HDF5-Datei\relax }}{44}{figure.caption.24}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Dataset mit Größe 1x10}}}{44}{subfigure.3.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Aufbau der Beispiel HDF5-Datei}}}{44}{subfigure.3.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Attribute von \textit {Classifications}}}}{44}{subfigure.3.3}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Vergleich der Darstellung von Perspektive und Tiefe zwischen C.LABEL und C.LABEL-VR\relax }}{47}{figure.caption.25}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {C.Label}}}{47}{subfigure.4.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {C.Label-VR}}}{47}{subfigure.4.2}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Beide Bilder zeigen Komponenten aus der Unity Editor, die dem Punkt-Prefab zugeordnet sind.\relax }}{48}{figure.caption.26}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Mesh Filter- und Collider-Komponente}}}{48}{subfigure.5.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Mesh Renderer-Komponente}}}{48}{subfigure.5.2}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Punktwolke in C.LABEL-VR mit mit Annotationen, die beim Einlesen der Daten schon vorhanden sind.\relax }}{50}{figure.caption.27}
\contentsline {figure}{\numberline {4.7}{\ignorespaces Oculus Touch Controller mit der offiziellen Beschriftung von Oculus \cite {bib:OculusTouchPic}\relax }}{54}{figure.caption.28}
\contentsline {figure}{\numberline {4.8}{\ignorespaces Zu sehen ist der Vorgang eines Pointer-Teleports.\relax }}{58}{figure.caption.29}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Vor dem Teleport}}}{58}{subfigure.8.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Nach dem Teleport}}}{58}{subfigure.8.2}
\contentsline {figure}{\numberline {4.9}{\ignorespaces Annotation eines Punktes mit der Klasse \textit {Car}, durch eine Pointer-Annotation\relax }}{60}{figure.caption.30}
\contentsline {figure}{\numberline {4.10}{\ignorespaces Abbildung eines Touch Annotations-Vorgang. Mit der linken Hand wird eine Punkt berührt, der dadurch klassifiziert wird.\relax }}{62}{figure.caption.31}
\contentsline {figure}{\numberline {4.11}{\ignorespaces Objektidentifizierung einer 3D-Punktwolke aus \cite {bib:Segmentation1}. Die Bodenpunkte der Wolke sind blau dargestellt und die Objekte sind mit Boxen gekennzeichnet.\relax }}{64}{figure.caption.32}
\contentsline {figure}{\numberline {4.12}{\ignorespaces In \cite {bib:Segmentation1} wird gezeigt, dass die Segmentierung einer Punktwolke zu einem besseren Ergebnis der Bodenpunktanalyse führt. Die obere Abbildung ist dabei ohne Segmentierung und die untere mit Segmentierung. Bodenpunkte sind blau und Objektpunkte in grün dargestellt. Die roten Rechtecke kennzeichnen den Bereich in dem die Analyse schlechter bzw. besser ist.\relax }}{65}{figure.caption.33}
\contentsline {figure}{\numberline {4.13}{\ignorespaces Vergleich der Segmentierungsmethoden zur Bodenpunktanalyse aus \cite {bib:Segmentation1} und C.LABEL-VR. Bodenpunkte sind rot, alle anderen schwarz. Die blauen Linien stellen die Trennung der Segmente dar. Die grünen Boxen markieren den Bereich, in dem man den Unterschied der beiden Methoden bei der Bodenpunkterkennung sieht.\relax }}{66}{figure.caption.34}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Segmentierung anhand potentieller Bodenpunkten}}}{66}{subfigure.13.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Segmentierung anhand Koordinaten entlang der $x$-Achse}}}{66}{subfigure.13.2}
\contentsline {figure}{\numberline {4.14}{\ignorespaces Bei zu wenigen Bodenpunkten in einem Segment passt sich die Ebene den falschen Punkten an.\relax }}{66}{figure.caption.35}
\contentsline {figure}{\numberline {4.15}{\ignorespaces Klassendiagramm des UI-Systems in C.LABEL-VR\relax }}{72}{figure.caption.36}
\contentsline {figure}{\numberline {4.16}{\ignorespaces Geöffnetes Applikationsmenü innerhalb einer Punktwolke\relax }}{74}{figure.caption.37}
\contentsline {figure}{\numberline {4.17}{\ignorespaces Die Abbildungen zeigen die zwei Benutzeroberflächen, die innerhalb einer Session in C.LABEL-VR aufgerufen werden können.\relax }}{75}{figure.caption.38}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Bewegungsmenü}}}{75}{subfigure.17.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Klassifikationsmenü}}}{75}{subfigure.17.2}
\contentsline {figure}{\numberline {4.18}{\ignorespaces Benutzeroberfläche zum erstellen oder editieren einer Klassifikation\relax }}{77}{figure.caption.39}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
